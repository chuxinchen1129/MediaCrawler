#!/usr/bin/env python3
"""
æ•°æ®åˆ†ææŠ¥å‘Š - é“¶é¥°å“ ç™½å²š å°çº¢ä¹¦æ•°æ®
ç”Ÿæˆè¯¦ç»†çš„æ•°æ®åˆ†ææŠ¥å‘Š
"""

import pandas as pd
import json
from datetime import datetime
from collections import Counter

def load_data():
    """åŠ è½½æ¸…æ´—åçš„Excelæ•°æ®"""
    excel_file = '/Users/echochen/MediaCrawler/data/xhs/é“¶é¥°å“ç™½å²š_æ¸…æ´—æ•°æ®.xlsx'
    df = pd.read_excel(excel_file)
    print(f"âœ“ åŠ è½½äº† {len(df)} æ¡æ•°æ®")
    # æ·»åŠ "å†…å®¹æ‘˜è¦"åˆ—ï¼Œå¦‚æœä¸å­˜åœ¨
    if 'å†…å®¹æ‘˜è¦' not in df.columns:
        df['å†…å®¹æ‘˜è¦'] = df['å†…å®¹æ‘˜è¦'] if 'å†…å®¹æ‘˜è¦' in df.columns else ''
    return df

def analyze_brand_mentions(df):
    """åˆ†æå“ç‰ŒæåŠæƒ…å†µ"""
    print("\n" + "="*60)
    print("å“ç‰ŒæåŠåˆ†æ")
    print("="*60)

    # ç»Ÿè®¡æ ‡é¢˜ä¸­åŒ…å«"ç™½å²š"çš„ç¬”è®°
    brand_mentions = df[df['æ ‡é¢˜'].str.contains('ç™½å²š', na=False)]
    print(f"æ ‡é¢˜ä¸­ç›´æ¥æåŠ'ç™½å²š'çš„ç¬”è®°: {len(brand_mentions)} æ¡")

    # ç»Ÿè®¡æ ‡ç­¾ä¸­åŒ…å«"ç™½å²š"çš„ç¬”è®°
    tag_mentions = df[df['æ ‡ç­¾'].str.contains('ç™½å²š', na=False)]
    print(f"æ ‡ç­¾ä¸­åŒ…å«'ç™½å²š'çš„ç¬”è®°: {len(tag_mentions)} æ¡")

    # ç»Ÿè®¡å†…å®¹ä¸­åŒ…å«"ç™½å²š"çš„ç¬”è®°
    desc_mentions = df[df['å†…å®¹æ‘˜è¦'].str.contains('ç™½å²š', na=False)]
    print(f"å†…å®¹æ‘˜è¦ä¸­åŒ…å«'ç™½å²š'çš„ç¬”è®°: {len(desc_mentions)} æ¡")

    # ä»»ä½•ä½ç½®æåŠç™½å²š
    any_mention = df[
        df['æ ‡é¢˜'].str.contains('ç™½å²š', na=False) |
        df['æ ‡ç­¾'].str.contains('ç™½å²š', na=False) |
        df['å†…å®¹æ‘˜è¦'].str.contains('ç™½å²š', na=False)
    ]
    print(f"ä»»ä½•ä½ç½®æåŠ'ç™½å²š'çš„ç¬”è®°: {len(any_mention)} æ¡ ({len(any_mention)/len(df)*100:.1f}%)")

    return any_mention

def analyze_engagement(df):
    """åˆ†æäº’åŠ¨æ•°æ®"""
    print("\n" + "="*60)
    print("äº’åŠ¨æ•°æ®åˆ†æ")
    print("="*60)

    # ç‚¹èµæ•°åˆ†å¸ƒ
    print("\nç‚¹èµæ•°åˆ†å¸ƒ:")
    print(f"  å¹³å‡ç‚¹èµæ•°: {df['ç‚¹èµæ•°'].mean():.1f}")
    print(f"  ä¸­ä½æ•°ç‚¹èµæ•°: {df['ç‚¹èµæ•°'].median():.0f}")
    print(f"  æœ€é«˜ç‚¹èµæ•°: {df['ç‚¹èµæ•°'].max()}")
    print(f"  æœ€ä½ç‚¹èµæ•°: {df['ç‚¹èµæ•°'].min()}")

    # é«˜äº’åŠ¨ç¬”è®°ï¼ˆç‚¹èµ>1000ï¼‰
    high_engagement = df[df['ç‚¹èµæ•°'] > 1000]
    print(f"\né«˜äº’åŠ¨ç¬”è®°ï¼ˆç‚¹èµ>1000ï¼‰: {len(high_engagement)} æ¡")
    if len(high_engagement) > 0:
        print("  TOP 5 é«˜äº’åŠ¨ç¬”è®°:")
        top_liked = high_engagement.nlargest(5, 'ç‚¹èµæ•°')[['æ ‡é¢˜', 'ç‚¹èµæ•°', 'æ”¶è—æ•°', 'è¯„è®ºæ•°']]
        for idx, row in top_liked.iterrows():
            print(f"    {row['æ ‡é¢˜'][:40]}... - {row['ç‚¹èµæ•°']}èµ")

    # æ”¶è—æ•°åˆ†å¸ƒ
    print(f"\næ”¶è—æ•°åˆ†å¸ƒ:")
    print(f"  å¹³å‡æ”¶è—æ•°: {df['æ”¶è—æ•°'].mean():.1f}")
    print(f"  æœ€é«˜æ”¶è—æ•°: {df['æ”¶è—æ•°'].max()}")

def analyze_content_types(df):
    """åˆ†æå†…å®¹ç±»å‹"""
    print("\n" + "="*60)
    print("å†…å®¹ç±»å‹åˆ†æ")
    print("="*60)

    # ç¬”è®°ç±»å‹
    type_counts = df['ç±»å‹'].value_counts()
    print(f"\nç¬”è®°ç±»å‹åˆ†å¸ƒ:")
    for note_type, count in type_counts.items():
        print(f"  {note_type}: {count} æ¡ ({count/len(df)*100:.1f}%)")

    # è§†é¢‘å†…å®¹
    has_video = df[df['æ˜¯å¦æœ‰è§†é¢‘'] == 'æ˜¯']
    print(f"\nåŒ…å«è§†é¢‘çš„ç¬”è®°: {len(has_video)} æ¡ ({len(has_video)/len(df)*100:.1f}%)")

    # å›¾ç‰‡æ•°é‡åˆ†å¸ƒ
    print(f"\nå›¾ç‰‡æ•°é‡åˆ†å¸ƒ:")
    print(f"  å¹³å‡æ¯æ¡ç¬”è®°å›¾ç‰‡æ•°: {df['å›¾ç‰‡æ•°é‡'].mean():.1f}")
    print(f"  æœ€å¤šå›¾ç‰‡æ•°: {df['å›¾ç‰‡æ•°é‡'].max()}")

def analyze_time_distribution(df):
    """åˆ†ææ—¶é—´åˆ†å¸ƒ"""
    print("\n" + "="*60)
    print("æ—¶é—´åˆ†å¸ƒåˆ†æ")
    print("="*60)

    # è½¬æ¢å‘å¸ƒæ—¥æœŸ
    df['å‘å¸ƒæ—¥æœŸ'] = pd.to_datetime(df['å‘å¸ƒæ—¥æœŸ'], errors='coerce')

    # æŒ‰æ—¥æœŸæ’åº
    df_sorted = df.sort_values('å‘å¸ƒæ—¥æœŸ', ascending=False)

    print(f"\næ—¶é—´è·¨åº¦:")
    if df_sorted['å‘å¸ƒæ—¥æœŸ'].notna().any():
        print(f"  æœ€æ—©å‘å¸ƒ: {df_sorted['å‘å¸ƒæ—¥æœŸ'].min().strftime('%Y-%m-%d')}")
        print(f"  æœ€æ–°å‘å¸ƒ: {df_sorted['å‘å¸ƒæ—¥æœŸ'].max().strftime('%Y-%Y-%m-%d')}")

    # æŒ‰å¹´ä»½ç»Ÿè®¡
    df['å¹´ä»½'] = df['å‘å¸ƒæ—¥æœŸ'].dt.year
    year_counts = df['å¹´ä»½'].value_counts().sort_index()
    print(f"\næŒ‰å¹´ä»½åˆ†å¸ƒ:")
    for year, count in year_counts.items():
        print(f"  {int(year)}å¹´: {count} æ¡")

def analyze_creators(df):
    """åˆ†æåˆ›ä½œè€…"""
    print("\n" + "="*60)
    print("åˆ›ä½œè€…åˆ†æ")
    print("="*60)

    # Topåˆ›ä½œè€…ï¼ˆæŒ‰ç¬”è®°æ•°é‡ï¼‰
    creator_counts = df['æ˜µç§°'].value_counts()
    print(f"\næ´»è·ƒåˆ›ä½œè€…ï¼ˆTOP 10ï¼‰:")
    for i, (creator, count) in enumerate(creator_counts.head(10).items(), 1):
        # è·å–è¯¥åˆ›ä½œè€…çš„æ€»äº’åŠ¨æ•°
        creator_data = df[df['æ˜µç§°'] == creator]
        total_likes = creator_data['ç‚¹èµæ•°'].sum()
        print(f"  {i}. {creator}: {count} æ¡ç¬”è®° (æ€»ç‚¹èµ {total_likes:,})")

def generate_markdown_report(df, output_file):
    """ç”ŸæˆMarkdownæŠ¥å‘Š"""
    print(f"\næ­£åœ¨ç”ŸæˆæŠ¥å‘Š...")

    report = f"""# é“¶é¥°å“ ç™½å²š - å°çº¢ä¹¦æ•°æ®åˆ†ææŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æ•°æ®æ¥æº**: å°çº¢ä¹¦
**å…³é”®è¯**: é“¶é¥°å“ ç™½å²š

---

## ğŸ“Š æ•°æ®æ¦‚è§ˆ

### é‡‡é›†ç»Ÿè®¡
- **æ€»ç¬”è®°æ•°**: {len(df)} æ¡
- **å›¾æ–‡ç¬”è®°**: {len(df[df['ç±»å‹'] == 'normal'])} æ¡
- **è§†é¢‘ç¬”è®°**: {len(df[df['ç±»å‹'] == 'video'])} æ¡
- **åŒ…å«è§†é¢‘**: {len(df[df['æ˜¯å¦æœ‰è§†é¢‘'] == 'æ˜¯'])} æ¡

### äº’åŠ¨æ•°æ®æ±‡æ€»
- **æ€»ç‚¹èµæ•°**: {df['ç‚¹èµæ•°'].sum():,}
- **æ€»æ”¶è—æ•°**: {df['æ”¶è—æ•°'].sum():,}
- **æ€»è¯„è®ºæ•°**: {df['è¯„è®ºæ•°'].sum():,}
- **æ€»åˆ†äº«æ•°**: {df['åˆ†äº«æ•°'].sum():,}

### å¹³å‡æ•°æ®
- **å¹³å‡ç‚¹èµæ•°**: {df['ç‚¹èµæ•°'].mean():.1f}
- **å¹³å‡æ”¶è—æ•°**: {df['æ”¶è—æ•°'].mean():.1f}
- **å¹³å‡è¯„è®ºæ•°**: {df['è¯„è®ºæ•°'].mean():.1f}

---

## ğŸ“ˆ å†…å®¹åˆ†æ

### ç¬”è®°ç±»å‹åˆ†å¸ƒ
"""

    # ç¬”è®°ç±»å‹
    type_counts = df['ç±»å‹'].value_counts()
    for note_type, count in type_counts.items():
        report += f"- **{note_type}**: {count} æ¡ ({count/len(df)*100:.1f}%)\n"

    report += f"""
### åª’ä½“å†…å®¹
- **åŒ…å«å›¾ç‰‡çš„ç¬”è®°**: {len(df[df['å›¾ç‰‡æ•°é‡'] > 0])} æ¡
- **åŒ…å«è§†é¢‘çš„ç¬”è®°**: {len(df[df['æ˜¯å¦æœ‰è§†é¢‘'] == 'æ˜¯'])} æ¡
- **å¹³å‡å›¾ç‰‡æ•°**: {df['å›¾ç‰‡æ•°é‡'].mean():.1f} å¼ /æ¡

---

## ğŸ”¥ çƒ­é—¨å†…å®¹

### é«˜äº’åŠ¨ç¬”è®°ï¼ˆç‚¹èµTOP 10ï¼‰
"""

    # TOP 10 ç‚¹èµç¬”è®°
    top_liked = df.nlargest(10, 'ç‚¹èµæ•°')
    for i, row in top_liked.iterrows():
        report += f"\n{i+1}. **{row['æ ‡é¢˜'][:50]}...**\n"
        report += f"   - ç‚¹èµ: {row['ç‚¹èµæ•°']:,} | æ”¶è—: {row['æ”¶è—æ•°']:,} | è¯„è®º: {row['è¯„è®ºæ•°']:,}\n"
        report += f"   - ä½œè€…: {row['æ˜µç§°']}\n"

    report += f"""
---

## ğŸ‘¥ åˆ›ä½œè€…åˆ†æ

### æ´»è·ƒåˆ›ä½œè€…ï¼ˆç¬”è®°æ•°é‡ TOP 10ï¼‰
"""

    # Topåˆ›ä½œè€…
    creator_counts = df['æ˜µç§°'].value_counts().head(10)
    for i, (creator, count) in creator_counts.items():
        creator_data = df[df['æ˜µç§°'] == creator]
        total_likes = creator_data['ç‚¹èµæ•°'].sum()
        report += f"\n{i+1}. **{creator}**\n"
        report += f"   - ç¬”è®°æ•°: {count} æ¡\n"
        report += f"   - æ€»ç‚¹èµ: {total_likes:,}\n"

    report += f"""
---

## ğŸ“… æ—¶é—´åˆ†å¸ƒ

### å‘å¸ƒå¹´ä»½ç»Ÿè®¡
"""

    # æŒ‰å¹´ä»½ç»Ÿè®¡
    df['å‘å¸ƒæ—¥æœŸ'] = pd.to_datetime(df['å‘å¸ƒæ—¥æœŸ'], errors='coerce')
    df['å¹´ä»½'] = df['å‘å¸ƒæ—¥æœŸ'].dt.year
    year_counts = df['å¹´ä»½'].value_counts().sort_index(ascending=False)
    for year, count in year_counts.items():
        report += f"- **{int(year)}å¹´**: {count} æ¡\n"

    report += f"""
---

## ğŸ“ æ•°æ®æ–‡ä»¶

- **Excelæ–‡ä»¶**: `/Users/echochen/MediaCrawler/data/xhs/é“¶é¥°å“ç™½å²š_æ¸…æ´—æ•°æ®.xlsx`
- **JSONæ–‡ä»¶**: `/Users/echochen/MediaCrawler/data/xhs/json/search_contents_2026-01-31.json`
- **å›¾ç‰‡ç›®å½•**: `/Users/echochen/MediaCrawler/data/xhs/images/`
- **è§†é¢‘ç›®å½•**: `/Users/echochen/MediaCrawler/data/xhs/videos/`

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**æ•°æ®é‡‡é›†å·¥å…·**: MediaCrawler
**åˆ†æè„šæœ¬**: clean_bailan_data.py + generate_report.py
"""

    # ä¿å­˜æŠ¥å‘Š
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)

    print(f"âœ“ MarkdownæŠ¥å‘Šå·²ä¿å­˜: {output_file}")

def main():
    """ä¸»æµç¨‹"""
    print("="*60)
    print("é“¶é¥°å“ç™½å²š - å°çº¢ä¹¦æ•°æ®åˆ†ææŠ¥å‘Šç”Ÿæˆ")
    print("="*60)

    # 1. åŠ è½½æ•°æ®
    print("\n[1/3] åŠ è½½æ•°æ®...")
    df = load_data()

    # 2. åˆ†ææ•°æ®
    print("\n[2/3] åˆ†ææ•°æ®...")
    analyze_brand_mentions(df)
    analyze_engagement(df)
    analyze_content_types(df)
    analyze_time_distribution(df)
    analyze_creators(df)

    # 3. ç”ŸæˆæŠ¥å‘Š
    print("\n[3/3] ç”ŸæˆæŠ¥å‘Š...")
    report_file = '/Users/echochen/MediaCrawler/data/xhs/é“¶é¥°å“ç™½å²š_åˆ†ææŠ¥å‘Š.md'
    generate_markdown_report(df, report_file)

    print("\n" + "="*60)
    print("âœ… åˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
    print("="*60)
    print(f"\næŠ¥å‘Šæ–‡ä»¶: {report_file}")

if __name__ == "__main__":
    main()
